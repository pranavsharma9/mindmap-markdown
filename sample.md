# Hallucination Mitigation in Vision-Language Models (LVLMs)\n## Challenges\n- Hallucination: Misalignment between visual inputs and textual outputs\n- Suppression of visual information by strong language priors\n- Sensitivity of text decoders to vision inputs\n\n## Techniques\n### FIHA: Fine-grained Hallucination Evaluation\n- Autonomous and cost-effective evaluation using Davidson Scene Graphs (DSG)\n- Models dependencies among hallucination types\n- Generates Q&A pairs for image datasets like MSCOCO and Foggy\n- Benchmark: FIHA-v1 for diverse questions\n\n### Dynamic Correction Decoding (DeCo)\n- Adjusts output logits by integrating preceding layer knowledge\n- Model-agnostic and seamlessly incorporates with classic decoding strategies\n- Reduces hallucination rates significantly\n\n### Latent Space Steering (VTI)\n- Stabilizes vision features by steering latent space representations\n- Task-agnostic, applied during inference without additional cost\n- Reduces hallucinations effectively across benchmarks\n\n### HALC: Adaptive Focal-Contrast Decoding\n- Mitigates Object Hallucinations (OH) via auto-focal grounding (local) and specialized beam search (global)\n- Plug-and-play module for LVLMs without extra training\n- Outperforms state-of-the-art in OH reduction across benchmarks\n\n## Findings\n- FIHA emphasizes dependencies and evaluation benchmarks.\n- DeCo leverages layer-specific knowledge to refine decoding.\n- VTI highlights stability of vision features in latent space.\n- HALC combines local grounding and global optimization to reduce hallucination.
